{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Random Forests\n",
    "## Hudson Arney & Ian Golvach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Social_Network_Ads.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='Age', y='EstimatedSalary', hue='Purchased')\n",
    "plt.title('Age vs Estimated Salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch implemntation of RF\n",
    "Learning sourced from: https://en.wikipedia.org/wiki/Random_forest\n",
    "\n",
    "Based on the wikipedia article, we will need to modify a standard decision tree model so that\n",
    " - Along with the gini coef. split, we select a random subset of the feature to consider when making splits\n",
    " - Several decision trees are trained with a 'bagging' approach, training the trees on a random swample w/ replacement of the data.\n",
    " \n",
    "Following this, it makes the most sense that we first implement a method that will handle the sampling and training, as well as the predicting via plurality, and another method that will server as the model itself for the underlying decision trees (that way we can test the bagging concept with other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "class bagging_trees:\n",
    "    \"\"\"Bootstrap aggregates a decision tree, bagging B times\"\"\"\n",
    "    def __init__(self, model, B=5, is_classifier=True):\n",
    "        self.B = B\n",
    "        self.model = model\n",
    "        self.fitted_models = []\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Create B models fitted on n sets created by randomly sampling with replacement from X and y\n",
    "        if(X.shape[0] != y.shape[0]):\n",
    "            raise Exception(\"X and y do not have the same number of columns\")\n",
    "        for i in range(self.B):\n",
    "            self.__progress(i)\n",
    "            bag_X = []\n",
    "            bag_y = []\n",
    "            for i in range(X.shape[0]):\n",
    "                sample_ind = np.random.randint(X.shape[0])\n",
    "                bag_X.append(X[sample_ind])\n",
    "                bag_y.append(y[sample_ind])\n",
    "            new_model = self.model()\n",
    "            new_model.fit(np.array(bag_X), np.array(bag_y))\n",
    "            self.fitted_models.append(new_model)\n",
    "        self.__progress(self.B+1)\n",
    "            \n",
    "    def predict(self, X_pred):\n",
    "        # Return the average of predictions if regression, otherwise return plurality of predictions\n",
    "        # It may be advantageous to later replace these with np functions\n",
    "        predictions = []\n",
    "        for i in range(self.B):\n",
    "            predictions.append(self.fitted_models[i].predict(X_pred))\n",
    "        if(self.is_classifier):\n",
    "            return np.ravel(scipy.stats.mode(np.array(predictions).T,1,keepdims=False)[0])\n",
    "        else:\n",
    "            return np.mean(np.array(predictions).T,1)\n",
    "        \n",
    "    def __progress(self, current):\n",
    "        if(current-1 == self.B):\n",
    "            print(\"[=] Sub-model \"+str(self.B)+'/'+str(self.B)+' fitted. Fitting complete!')\n",
    "        else:\n",
    "            spinner = ' '\n",
    "            # |/-\\\n",
    "            spinny = current%4\n",
    "            if(spinny==0):\n",
    "                spinner = '|'\n",
    "            elif(spinny==1):\n",
    "                spinner = '/'\n",
    "            elif(spinny==2):\n",
    "                spinner = '-'\n",
    "            elif(spinny==3):\n",
    "                spinner = '\\\\'\n",
    "            print(\"[\"+spinner+\"] Sub-model \"+str(current+1)+'/'+str(self.B)+' fitting...', end='\\r')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is just a proof of concept that the class actually works, which is why train/test splitting was not performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_trees = bagging_trees(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['Age','EstimatedSalary']])\n",
    "y= np.array(df['Purchased'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement a decision tree that picks features at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = [1,3,4,2,5]\n",
    "test_array.sort()\n",
    "test_array.remove(2)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class rdt_node:\n",
    "    #type(X)==np.ndarray\n",
    "    \"\"\"\n",
    "    Node for random_decision_tree, has a boundary, variable number, and between 0 and 2 children\n",
    "    Children of this node can either be another node, or a value.\n",
    "       \n",
    "    var_num: the variable that is compared at this node\n",
    "    boundary: the boundary along the variable, <= is left, otherwise right\n",
    "    left: the left child of the node, or the value of the leaf\n",
    "    right: the right child of the node, or the value of the leaf\n",
    "    \"\"\"\n",
    "    def __init__(self, var_num=0, boundary=0, left=None, right=None):\n",
    "        self.var_num = var_num\n",
    "        self.boundary = boundary\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class random_decision_tree:\n",
    "    \"\"\"\n",
    "    Decision Tree that follows random forest standards\n",
    "    \n",
    "    depth_limit: how deep the forest is allowed to go before stopping\n",
    "    feature_subset_func: the function used to determine how many feature to use based on number of features\n",
    "    \"\"\"\n",
    "    def __init__(self, depth_limit=5, feature_subset_func=lambda x: math.floor(pow(x,1/2))):\n",
    "        self.depth_limit = 5\n",
    "        self.root = rdt_node();\n",
    "        self.num_features = -1;\n",
    "        self.feature_subset_func = feature_subset_func\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        #X_train is a 2D numpy ndarray of shape (observations, features)\n",
    "        #y_train is a 1D numpy array of the true value for the feature\n",
    "        self.num_features = X_train.shape[1]\n",
    "        self.__fit(X_train, y_train, 0, self.root)\n",
    "        \n",
    "        \n",
    "    def __fit(self, X_train, y_train, depth, node):\n",
    "        #for subsequent _fits, pass in the subset that is on that side of the boudary\n",
    "        #depth increments each time fit is called\n",
    "        #if the depth==depth_limit, do not call _fit again, but assign the plurality label and/or average value to child\n",
    "        \n",
    "        # first, decide which random subset of variables you will use for calculating the boundary\n",
    "        feature_candidates = self.__array_for_range(X_train.shape[1])\n",
    "        features = []\n",
    "        \n",
    "        for i in range(self.feature_subset_func(X_train.shape[1])):\n",
    "            index = np.random.randint(len(feature_candidates))\n",
    "            features.append(feature_candidates[index])\n",
    "            feature_candidates.remove(feature_candidates[index])\n",
    "        \n",
    "        \n",
    "        # then, find the desirable boundary for using gini coef (sub-method)\n",
    "        gini_result = self.__find_gini(X_train, y_train, features)\n",
    "        # if at depth, assign leafs, otherwise call fit on the subsets of the new boundary\n",
    "        node.var_num = gini_result[1]\n",
    "        node.boundary = gini_result[0]\n",
    "        if(self.depth_limit == depth):\n",
    "            #assign leaves based on plurity feature / average feature (class/regress)\n",
    "            #classification assumed currently\n",
    "            node.left = scipy.stats.mode(y_train[X_train[:,node.var_num]<=node.boundary],keepdims=False)[0]\n",
    "            node.right = scipy.stats.mode(y_train[X_train[:,node.var_num]>node.boundary],keepdims=False)[0]\n",
    "        else:\n",
    "            # if leafs pure, assign their unique value, else call fit\n",
    "            if(gini_result[2] and gini_result[3]):\n",
    "                # both are leaves\n",
    "                node.left = scipy.stats.mode(y_train[X_train[:,node.var_num]<=node.boundary],keepdims=False)[0]\n",
    "                node.right = scipy.stats.mode(y_train[X_train[:,node.var_num]>node.boundary],keepdims=False)[0]\n",
    "            elif(gini_result[2]):\n",
    "                # only left is leaf, call fit on right\n",
    "                node.left = scipy.stats.mode(y_train[X_train[:,node.var_num]<=node.boundary],keepdims=False)[0]\n",
    "                node.right = rdt_node()\n",
    "                self.__fit(X_train[X_train[:,node.var_num]>node.boundary],y_train[X_train[:,node.var_num]>node.boundary], depth + 1, node.right)\n",
    "            elif(gini_result[3]):\n",
    "                # only right is leaf, call fit on left\n",
    "                node.right = scipy.stats.mode(y_train[X_train[:,node.var_num]>node.boundary],keepdims=False)[0]\n",
    "                node.left = rdt_node()\n",
    "                self.__fit(X_train[X_train[:,node.var_num]<=node.boundary],y_train[X_train[:,node.var_num]<=node.boundary], depth + 1, node.left)\n",
    "            else:\n",
    "                # no leaves, call fit on both\n",
    "                node.left = rdt_node()\n",
    "                node.right = rdt_node()\n",
    "                self.__fit(X_train[X_train[:,node.var_num]<=node.boundary],y_train[X_train[:,node.var_num]<=node.boundary], depth + 1, node.left)\n",
    "                self.__fit(X_train[X_train[:,node.var_num]>node.boundary],y_train[X_train[:,node.var_num]>node.boundary], depth + 1, node.right)\n",
    "        # REMEMBER, _fit is called with the subset of the training X and y data that is on the respective side of the boundary\n",
    "        \n",
    "    def __array_for_range(self, num_for_array):\n",
    "        # Sometimes the simple solution works\n",
    "        ret = []\n",
    "        for i in range(num_for_array):\n",
    "            ret.append(i)\n",
    "        return ret\n",
    "    \n",
    "    def __find_gini(self, X_train, y_train, features):\n",
    "        # features is a list of integers denoting which column numbers (feature) to consider\n",
    "        # returns a tuple of (boundary, feature, left_leaf, right_leaf) of types (int,float,bool,bool)\n",
    "        # left_leaf, right_leaf are returned true if everything on the side of a boundary has the same y value\n",
    "        unincluded_features = self.__array_for_range(X_train.shape[1])\n",
    "        for x in features:\n",
    "            unincluded_features.remove(x)\n",
    "        #candidates are of form (boundary,feature), they are the halfway point between all parts of a feature\n",
    "        boundary_candidates = []\n",
    "        #boundaries are of form (boundary,feature,gini_coef)\n",
    "        boundaries = []\n",
    "        \n",
    "        #setting up candidates\n",
    "        #sorted_values = None\n",
    "        for i in features:\n",
    "            possible_values = np.unique(X_train[:,i])\n",
    "            sorted_values = np.sort(possible_values)\n",
    "            #print(sorted_values)\n",
    "            #print(sorted_values.shape)\n",
    "            #print(sorted_values.shape)\n",
    "            if(sorted_values.shape[0] == 2):\n",
    "                boundary_candidates.append(((sorted_values[0]+sorted_values[1])/2,i))\n",
    "            else:\n",
    "                for j in range(sorted_values.shape[0]-1):\n",
    "                    boundary_candidates.append(((sorted_values[j]+sorted_values[j+1])/2,i))\n",
    "        while(len(boundary_candidates)==0):\n",
    "            if(len(unincluded_features)==0):\n",
    "                raise Exception(\"Multiple duplicate observations, boundary cannot be found\")\n",
    "            #print(\"NO CANDIDATES\")\n",
    "            #print(X_train)\n",
    "            #print(y_train)\n",
    "            #print(sorted_values)\n",
    "            # Testing has indicated that this is a result of no variance on the chosen features\n",
    "            # As the sklearn decision tree does, we must allow more features until we atleast one candidate\n",
    "            i = unincluded_features[0]\n",
    "            possible_values = np.unique(X_train[:,i])\n",
    "            sorted_values = np.sort(possible_values)\n",
    "            #print(sorted_values.shape)\n",
    "            if(sorted_values.shape[0] == 2):\n",
    "                boundary_candidates.append(((sorted_values[0]+sorted_values[1])/2,i))\n",
    "            else:\n",
    "                for j in range(sorted_values.shape[0]-1):\n",
    "                    boundary_candidates.append(((sorted_values[i]+sorted_values[i+1])/2,i))\n",
    "            unincluded_features.remove(i)\n",
    "            \n",
    "        #print(boundary_candidates)\n",
    "        #Find gini for all candidates\n",
    "        for candidate in boundary_candidates:\n",
    "            boundaries.append((candidate[0],candidate[1],self.__gini_index_for_boundary(X_train, y_train, candidate[0], candidate[1])))\n",
    "        \n",
    "        #sort candidates using hidden method\n",
    "        boundaries.sort(key=self.__sorter)\n",
    "        \n",
    "        #set ret and check to see if either sub-region is 'pure'\n",
    "        best_boundary = boundaries[0]\n",
    "        left_leaf = (len(np.unique(y_train[X_train[:,best_boundary[1]]<=best_boundary[0]])) == 1)\n",
    "        right_leaf = (len(np.unique(y_train[X_train[:,best_boundary[1]]>best_boundary[0]])) == 1)\n",
    "        \n",
    "        return(best_boundary[0],best_boundary[1],left_leaf,right_leaf)\n",
    "        \n",
    "        \n",
    "    def __gini_index_for_boundary(self, X_train, y_train, boundary, feature_num):\n",
    "        #classification assumed\n",
    "        \n",
    "        #calculate proportions of feature_num <= boundary for 1, proportion of feature_num > boundary for 2\n",
    "        # code pulled from https://stackoverflow.com/questions/67586928/how-can-you-find-what-percentage-of-a-numpy-array-has-some-value-x\n",
    "        # uniques, counts = np.unique(array, return_counts=True)\n",
    "        # percentages = dict(zip(uniques, counts * 100 / len(array)))\n",
    "    \n",
    "        # r1 is the subset of y masked by X[:,feature_num]<= or > the boundary, because we use it to find the proportions\n",
    "        R_1 = y_train[X_train[:,feature_num]<=boundary]\n",
    "        N_1 = R_1.shape[0]\n",
    "        proportions_1 = np.unique(R_1,return_counts=True)[1]/N_1\n",
    "        \n",
    "        R_2 = y_train[X_train[:,feature_num]>boundary]\n",
    "        N_2 = R_2.shape[0]\n",
    "        proportions_2 = np.unique(R_2,return_counts=True)[1]/N_2\n",
    "        \n",
    "        N = X_train.shape[0]\n",
    "        \n",
    "        #compute ginis\n",
    "        g_1 = 0\n",
    "        for prop in proportions_1:\n",
    "            g_1 = g_1 + prop*(1-prop)\n",
    "            \n",
    "        g_2 = 0\n",
    "        for prop in proportions_2:\n",
    "            g_2 = g_2 + prop*(1-prop)\n",
    "            \n",
    "        #return weighted average of ginis\n",
    "        return ((N_1/N)*(g_1))+((N_2/N)*(g_2))\n",
    "        \n",
    "    def __sorter(self, the_tuple):\n",
    "        return the_tuple[2]\n",
    "        \n",
    "    def predict(self, X_pred):\n",
    "        return np.array([self.__predict(x) for x in X_pred])\n",
    "    \n",
    "    def __predict(self, x_pred):\n",
    "        # For any vector of features given, transverses the tree in order to return the value\n",
    "        # x_pred is a vector\n",
    "        tolkein = self.root # could replace this with 'walker' or something else\n",
    "        while(type(tolkein)==type(self.root)):\n",
    "            if(x_pred[tolkein.var_num] <= tolkein.boundary):\n",
    "                tolkein = tolkein.left\n",
    "            else:\n",
    "                tolkein = tolkein.right\n",
    "        return tolkein\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "rdt = random_decision_tree()\n",
    "rdt.fit(X,y)\n",
    "predicted = rdt.predict(X)\n",
    "print(accuracy_score(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdt.root.var_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest = bagging_trees(random_decision_tree, 100)\n",
    "random_forest.fit(X,y)\n",
    "prediction = random_forest.predict(X)\n",
    "print(accuracy_score(y,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
