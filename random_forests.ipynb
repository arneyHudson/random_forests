{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Random Forests\n",
    "## Hudson Arney & Ian Golvach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Social_Network_Ads.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#0000ff', '#00ff00']\n",
    "cmap = ListedColormap(colors)\n",
    "plt.scatter(data=df, x='Age', y='EstimatedSalary', c='Purchased', cmap=cmap)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('Age vs Estimated Salary')\n",
    "\n",
    "legend_labels = ['Not Purchased', 'Purchased']\n",
    "legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#0000ff', markersize=10),\n",
    "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#00ff00', markersize=10)]\n",
    "plt.legend(legend_markers, legend_labels, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Scratch implemntation of RF\n",
    "Learning sourced from: https://en.wikipedia.org/wiki/Random_forest\n",
    "\n",
    "Based on the wikipedia article, we will need to modify a standard decision tree model so that\n",
    " - Along with the gini coef. split, we select a random subset of the feature to consider when making splits\n",
    " - Several decision trees are trained with a 'bagging' approach, training the trees on a random swample w/ replacement of the data.\n",
    " \n",
    "Following this, it makes the most sense that we first implement a method that will handle the sampling and training, as well as the predicting via plurality, and another method that will server as the model itself for the underlying decision trees (that way we can test the bagging concept with other models)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bagging_trees:\n",
    "    \"\"\"Bootstrap aggregates a decision tree, bagging B times\"\"\"\n",
    "    def __init__(self, model, B=5, is_classifier=True):\n",
    "        self.B = B\n",
    "        self.model = model\n",
    "        self.fitted_models = []\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Create B models fitted on n sets created by randomly sampling with replacement from X and y\n",
    "        if(X.shape[0] != y.shape[0]):\n",
    "            raise Exception(\"X and y do not have the same number of columns\")\n",
    "        for i in range(self.B):\n",
    "            self.__progress(i)\n",
    "            bag_X = []\n",
    "            bag_y = []\n",
    "            for i in range(X.shape[0]):\n",
    "                sample_ind = np.random.randint(X.shape[0])\n",
    "                bag_X.append(X[sample_ind])\n",
    "                bag_y.append(y[sample_ind])\n",
    "            new_model = self.model()\n",
    "            new_model.fit(np.array(bag_X), np.array(bag_y))\n",
    "            self.fitted_models.append(new_model)\n",
    "        self.__progress(self.B+1)\n",
    "            \n",
    "    def predict(self, X_pred):\n",
    "        # Return the average of predictions if regression, otherwise return plurality of predictions\n",
    "        # It may be advantageous to later replace these with np functions\n",
    "        predictions = []\n",
    "        for i in range(self.B):\n",
    "            prediction = self.fitted_models[i].predict(X_pred)\n",
    "            if np.issubdtype(prediction.dtype, np.number):  # Check if prediction is numeric\n",
    "                predictions.append(prediction)\n",
    "        if not predictions:  # Check if predictions list is empty\n",
    "            raise ValueError(\"No numeric predictions found\")\n",
    "        \n",
    "        predictions_array = np.array(predictions)  # Convert to numpy array\n",
    "        if self.is_classifier:\n",
    "            return np.ravel(scipy.stats.mode(predictions_array.T, axis=1, keepdims=False)[0])\n",
    "        else:\n",
    "            return np.mean(predictions_array.T, axis=1)\n",
    "        \n",
    "    def __progress(self, current):\n",
    "        if(current-1 == self.B):\n",
    "            print(\"[=] Sub-model \"+str(self.B)+'/'+str(self.B)+' fitted. Fitting complete!')\n",
    "        else:\n",
    "            spinner = ' '\n",
    "            # |/-\\\n",
    "            spinny = current%4\n",
    "            if(spinny==0):\n",
    "                spinner = '|'\n",
    "            elif(spinny==1):\n",
    "                spinner = '/'\n",
    "            elif(spinny==2):\n",
    "                spinner = '-'\n",
    "            elif(spinny==3):\n",
    "                spinner = '\\\\'\n",
    "            print(\"[\"+spinner+\"] Sub-model \"+str(current+1)+'/'+str(self.B)+' fitting...', end='\\r')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is just a proof of concept that the class actually works, which is why train/test splitting was not performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees = bagging_trees(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['Age','EstimatedSalary']])\n",
    "y= np.array(df['Purchased'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement a decision tree that picks features at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = [1,3,4,2,5]\n",
    "test_array.sort()\n",
    "test_array.remove(2)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rdt_node:\n",
    "    #type(X)==np.ndarray\n",
    "    \"\"\"\n",
    "    Node for random_decision_tree, has a boundary, variable number, and between 0 and 2 children\n",
    "    Children of this node can either be another node, or a value.\n",
    "       \n",
    "    var_num: the variable that is compared at this node\n",
    "    boundary: the boundary along the variable, <= is left, otherwise right\n",
    "    left: the left child of the node, or the value of the leaf\n",
    "    right: the right child of the node, or the value of the leaf\n",
    "    \"\"\"\n",
    "    def __init__(self, var_num=0, boundary=0, left=None, right=None):\n",
    "        self.var_num = var_num\n",
    "        self.boundary = boundary\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class random_decision_tree:\n",
    "    \"\"\"\n",
    "    Decision Tree that follows random forest standards\n",
    "    \n",
    "    depth_limit: how deep the forest is allowed to go before stopping\n",
    "    feature_subset_func: the function used to determine how many feature to use based on number of features\n",
    "    \"\"\"\n",
    "    def __init__(self, depth_limit=5, feature_subset_func=lambda x: math.floor(pow(x,1/2))):\n",
    "        self.depth_limit = 5\n",
    "        self.root = rdt_node();\n",
    "        self.num_features = -1;\n",
    "        self.feature_subset_func = feature_subset_func\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # Remove duplicate observations\n",
    "        unique_indices = np.unique(X_train, axis=0, return_index=True)[1]\n",
    "        X_train_unique = X_train[unique_indices]\n",
    "        y_train_unique = y_train[unique_indices]\n",
    "        \n",
    "        self.num_features = X_train_unique.shape[1]\n",
    "        self.__fit(X_train_unique, y_train_unique, 0, self.root)\n",
    "\n",
    "        \n",
    "        \n",
    "    def __fit(self, X_train, y_train, depth, node):\n",
    "        self.num_features = X_train.shape[1]\n",
    "        gini_result = self.__find_gini(X_train, y_train, self.__array_for_range(X_train.shape[1]))\n",
    "        if gini_result[0] is None:  # Check if boundary is None\n",
    "            return\n",
    "        node.var_num = gini_result[1]\n",
    "        node.boundary = gini_result[0]\n",
    "        \n",
    "        if self.depth_limit == depth or gini_result[2]:\n",
    "            node.left = scipy.stats.mode(y_train[X_train[:, node.var_num] <= node.boundary], keepdims=False)[0]\n",
    "            node.right = scipy.stats.mode(y_train[X_train[:, node.var_num] > node.boundary], keepdims=False)[0]\n",
    "        else:\n",
    "            node.left = rdt_node()\n",
    "            node.right = rdt_node()\n",
    "            self.__fit(X_train[X_train[:, node.var_num] <= node.boundary], y_train[X_train[:, node.var_num] <= node.boundary], depth + 1, node.left)\n",
    "            self.__fit(X_train[X_train[:, node.var_num] > node.boundary], y_train[X_train[:, node.var_num] > node.boundary], depth + 1, node.right)\n",
    "            \n",
    "    def __array_for_range(self, num_for_array):\n",
    "        # Sometimes the simple solution works\n",
    "        ret = []\n",
    "        for i in range(num_for_array):\n",
    "            ret.append(i)\n",
    "        return ret\n",
    "    \n",
    "    def __find_gini(self, X_train, y_train, features):\n",
    "        unincluded_features = self.__array_for_range(X_train.shape[1])\n",
    "        for x in features:\n",
    "            unincluded_features.remove(x)\n",
    "        \n",
    "        boundary_candidates = []\n",
    "        boundaries = []\n",
    "        \n",
    "        for feature_index in features:\n",
    "            unique_values = np.unique(X_train[:, feature_index])\n",
    "            \n",
    "            # Skip features with no variability\n",
    "            if len(unique_values) == 1:\n",
    "                continue\n",
    "            \n",
    "            if len(unique_values) == 2:\n",
    "                boundary_candidates.append((np.mean(unique_values), feature_index))\n",
    "            else:\n",
    "                for i in range(len(unique_values) - 1):\n",
    "                    boundary = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                    boundary_candidates.append((boundary, feature_index))\n",
    "        \n",
    "        if len(boundary_candidates) == 0:\n",
    "            return (None, None, False)\n",
    "            # raise Exception(\"No candidate boundaries found due to lack of variability in features\")\n",
    "        \n",
    "        for candidate in boundary_candidates:\n",
    "            boundaries.append((candidate[0], candidate[1], self.__gini_index_for_boundary(X_train, y_train, candidate[0], candidate[1])))\n",
    "            \n",
    "        boundaries.sort(key=self.__sorter)\n",
    "        \n",
    "        best_boundary = boundaries[0]\n",
    "        left_leaf = (len(np.unique(y_train[X_train[:, best_boundary[1]] <= best_boundary[0]])) == 1)\n",
    "        right_leaf = (len(np.unique(y_train[X_train[:, best_boundary[1]] > best_boundary[0]])) == 1)\n",
    "        is_leaf = left_leaf and right_leaf\n",
    "        \n",
    "        return (best_boundary[0], best_boundary[1], is_leaf)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def __gini_index_for_boundary(self, X_train, y_train, boundary, feature_num):\n",
    "        #classification assumed\n",
    "        \n",
    "        #calculate proportions of feature_num <= boundary for 1, proportion of feature_num > boundary for 2\n",
    "        # code pulled from https://stackoverflow.com/questions/67586928/how-can-you-find-what-percentage-of-a-numpy-array-has-some-value-x\n",
    "        # uniques, counts = np.unique(array, return_counts=True)\n",
    "        # percentages = dict(zip(uniques, counts * 100 / len(array)))\n",
    "    \n",
    "        # r1 is the subset of y masked by X[:,feature_num]<= or > the boundary, because we use it to find the proportions\n",
    "        R_1 = y_train[X_train[:,feature_num]<=boundary]\n",
    "        N_1 = R_1.shape[0]\n",
    "        proportions_1 = np.unique(R_1,return_counts=True)[1]/N_1\n",
    "        \n",
    "        R_2 = y_train[X_train[:,feature_num]>boundary]\n",
    "        N_2 = R_2.shape[0]\n",
    "        proportions_2 = np.unique(R_2,return_counts=True)[1]/N_2\n",
    "        \n",
    "        N = X_train.shape[0]\n",
    "        \n",
    "        #compute ginis\n",
    "        g_1 = 0\n",
    "        for prop in proportions_1:\n",
    "            g_1 = g_1 + prop*(1-prop)\n",
    "            \n",
    "        g_2 = 0\n",
    "        for prop in proportions_2:\n",
    "            g_2 = g_2 + prop*(1-prop)\n",
    "            \n",
    "        #return weighted average of ginis\n",
    "        return ((N_1/N)*(g_1))+((N_2/N)*(g_2))\n",
    "        \n",
    "    def __sorter(self, the_tuple):\n",
    "        return the_tuple[2]\n",
    "        \n",
    "    def predict(self, X_pred):\n",
    "        return np.array([self.__predict(x) for x in X_pred])\n",
    "    \n",
    "    def __predict(self, x_pred):\n",
    "        # For any vector of features given, transverses the tree in order to return the value\n",
    "        # x_pred is a vector\n",
    "        tolkein = self.root # could replace this with 'walker' or something else\n",
    "        while(type(tolkein)==type(self.root)):\n",
    "            if(x_pred[tolkein.var_num] <= tolkein.boundary):\n",
    "                tolkein = tolkein.left\n",
    "            else:\n",
    "                tolkein = tolkein.right\n",
    "        return tolkein\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdt = random_decision_tree()\n",
    "rdt.fit(X, y)\n",
    "predicted = rdt.predict(X)\n",
    "print(accuracy_score(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdt.root.var_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest = bagging_trees(random_decision_tree, 100)\n",
    "random_forest.fit(X, y)\n",
    "prediction = random_forest.predict(X)\n",
    "print(accuracy_score(y,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Plot the Decision Boundaries of the Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#87cefa', '#ff0000']\n",
    "cmap = ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=prediction, cmap=cmap, label='Predicted Labels')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('Predicted Labels')\n",
    "\n",
    "legend_labels = ['Not Purchased', 'Purchased']\n",
    "legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#87cefa', markersize=10),\n",
    "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#ff0000', markersize=10)]\n",
    "plt.legend(legend_markers, legend_labels, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.5),\n",
    "                     np.arange(y_min, y_max, 0.5))\n",
    "\n",
    "Z = rdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, s=20, edgecolor='k')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('Decision Boundary of Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
