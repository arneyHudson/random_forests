{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Random Forests\n",
    "## Hudson Arney & Ian Golvach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # For benchmark testing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import memory_profiler\n",
    "import psutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Social_Network_Ads.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#0000ff', '#00ff00']\n",
    "cmap = ListedColormap(colors)\n",
    "plt.scatter(data=df, x='Age', y='EstimatedSalary', c='Purchased', cmap=cmap)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('Age vs Estimated Salary')\n",
    "\n",
    "legend_labels = ['Not Purchased', 'Purchased']\n",
    "legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#0000ff', markersize=10),\n",
    "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#00ff00', markersize=10)]\n",
    "plt.legend(legend_markers, legend_labels, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Scratch implemntation of RF\n",
    "Learning sourced from: https://en.wikipedia.org/wiki/Random_forest\n",
    "\n",
    "Based on the wikipedia article, we will need to modify a standard decision tree model so that\n",
    " - Along with the gini coef. split, we select a random subset of the feature to consider when making splits\n",
    " - Several decision trees are trained with a 'bagging' approach, training the trees on a random swample w/ replacement of the data.\n",
    " \n",
    "Following this, it makes the most sense that we first implement a method that will handle the sampling and training, as well as the predicting via plurality, and another method that will server as the model itself for the underlying decision trees (that way we can test the bagging concept with other models)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bagging_trees:\n",
    "    \"\"\"Bootstrap aggregates a decision tree, bagging B times\"\"\"\n",
    "    def __init__(self, model, B=5, is_classifier=True):\n",
    "        self.B = B\n",
    "        self.model = model\n",
    "        self.fitted_models = []\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Create B models fitted on n sets created by randomly sampling with replacement from X and y\n",
    "        if(X.shape[0] != y.shape[0]):\n",
    "            raise Exception(\"X and y do not have the same number of columns\")\n",
    "        for i in range(self.B):\n",
    "            self.__progress(i)\n",
    "            bag_X = []\n",
    "            bag_y = []\n",
    "            for i in range(X.shape[0]):\n",
    "                sample_ind = np.random.randint(X.shape[0])\n",
    "                bag_X.append(X[sample_ind])\n",
    "                bag_y.append(y[sample_ind])\n",
    "            new_model = self.model()\n",
    "            new_model.fit(np.array(bag_X), np.array(bag_y))\n",
    "            self.fitted_models.append(new_model)\n",
    "        self.__progress(self.B+1)\n",
    "            \n",
    "    def predict(self, X_pred):\n",
    "        # Return the average of predictions if regression, otherwise return plurality of predictions\n",
    "        # It may be advantageous to later replace these with np functions\n",
    "        predictions = []\n",
    "        for i in range(self.B):\n",
    "            prediction = self.fitted_models[i].predict(X_pred)\n",
    "            if np.issubdtype(prediction.dtype, np.number):  # Check if prediction is numeric\n",
    "                predictions.append(prediction)\n",
    "        if not predictions:  # Check if predictions list is empty\n",
    "            raise ValueError(\"No numeric predictions found\")\n",
    "        \n",
    "        predictions_array = np.array(predictions)  # Convert to numpy array\n",
    "        if self.is_classifier:\n",
    "            return np.ravel(scipy.stats.mode(predictions_array.T, axis=1, keepdims=False)[0])\n",
    "        else:\n",
    "            return np.mean(predictions_array.T, axis=1)\n",
    "        \n",
    "    def __progress(self, current):\n",
    "        if(current-1 == self.B):\n",
    "            print(\"[=] Sub-model \"+str(self.B)+'/'+str(self.B)+' fitted. Fitting complete!')\n",
    "        else:\n",
    "            spinner = ' '\n",
    "            # |/-\\\n",
    "            spinny = current%4\n",
    "            if(spinny==0):\n",
    "                spinner = '|'\n",
    "            elif(spinny==1):\n",
    "                spinner = '/'\n",
    "            elif(spinny==2):\n",
    "                spinner = '-'\n",
    "            elif(spinny==3):\n",
    "                spinner = '\\\\'\n",
    "            print(\"[\"+spinner+\"] Sub-model \"+str(current+1)+'/'+str(self.B)+' fitting...', end='\\r')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is just a proof of concept that the class actually works, which is why train/test splitting was not performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees = bagging_trees(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['Age','EstimatedSalary']])\n",
    "y= np.array(df['Purchased'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_trees.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement a decision tree that picks features at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = [1,3,4,2,5]\n",
    "test_array.sort()\n",
    "test_array.remove(2)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rdt_node:\n",
    "    #type(X)==np.ndarray\n",
    "    \"\"\"\n",
    "    Node for random_decision_tree, has a boundary, variable number, and between 0 and 2 children\n",
    "    Children of this node can either be another node, or a value.\n",
    "       \n",
    "    var_num: the variable that is compared at this node\n",
    "    boundary: the boundary along the variable, <= is left, otherwise right\n",
    "    left: the left child of the node, or the value of the leaf\n",
    "    right: the right child of the node, or the value of the leaf\n",
    "    \"\"\"\n",
    "    def __init__(self, var_num=0, boundary=0, left=None, right=None):\n",
    "        self.var_num = var_num\n",
    "        self.boundary = boundary\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class random_decision_tree:\n",
    "    \"\"\"\n",
    "    Decision Tree that follows random forest standards\n",
    "    \n",
    "    depth_limit: how deep the forest is allowed to go before stopping\n",
    "    feature_subset_func: the function used to determine how many feature to use based on number of features\n",
    "    \"\"\"\n",
    "    def __init__(self, depth_limit=5, feature_subset_func=lambda x: math.floor(pow(x,1/2))):\n",
    "        self.depth_limit = 5\n",
    "        self.root = rdt_node();\n",
    "        self.num_features = -1;\n",
    "        self.feature_subset_func = feature_subset_func\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # Remove duplicate observations\n",
    "        unique_indices = np.unique(X_train, axis=0, return_index=True)[1]\n",
    "        X_train_unique = X_train[unique_indices]\n",
    "        y_train_unique = y_train[unique_indices]\n",
    "        \n",
    "        self.num_features = X_train_unique.shape[1]\n",
    "        self.__fit(X_train_unique, y_train_unique, 0, self.root)\n",
    "\n",
    "        \n",
    "        \n",
    "    def __fit(self, X_train, y_train, depth, node):\n",
    "        self.num_features = X_train.shape[1]\n",
    "        gini_result = self.__find_gini(X_train, y_train, self.__array_for_range(X_train.shape[1]))\n",
    "        if gini_result[0] is None:  # Check if boundary is None\n",
    "            return\n",
    "        node.var_num = gini_result[1]\n",
    "        node.boundary = gini_result[0]\n",
    "        \n",
    "        if self.depth_limit == depth or gini_result[2]:\n",
    "            node.left = scipy.stats.mode(y_train[X_train[:, node.var_num] <= node.boundary], keepdims=False)[0]\n",
    "            node.right = scipy.stats.mode(y_train[X_train[:, node.var_num] > node.boundary], keepdims=False)[0]\n",
    "        else:\n",
    "            node.left = rdt_node()\n",
    "            node.right = rdt_node()\n",
    "            self.__fit(X_train[X_train[:, node.var_num] <= node.boundary], y_train[X_train[:, node.var_num] <= node.boundary], depth + 1, node.left)\n",
    "            self.__fit(X_train[X_train[:, node.var_num] > node.boundary], y_train[X_train[:, node.var_num] > node.boundary], depth + 1, node.right)\n",
    "            \n",
    "    def __array_for_range(self, num_for_array):\n",
    "        # Sometimes the simple solution works\n",
    "        ret = []\n",
    "        for i in range(num_for_array):\n",
    "            ret.append(i)\n",
    "        return ret\n",
    "    \n",
    "    def __find_gini(self, X_train, y_train, features):\n",
    "        unincluded_features = self.__array_for_range(X_train.shape[1])\n",
    "        for x in features:\n",
    "            unincluded_features.remove(x)\n",
    "        \n",
    "        boundary_candidates = []\n",
    "        boundaries = []\n",
    "        \n",
    "        for feature_index in features:\n",
    "            unique_values = np.unique(X_train[:, feature_index])\n",
    "            \n",
    "            # Skip features with no variability\n",
    "            if len(unique_values) == 1:\n",
    "                continue\n",
    "            \n",
    "            if len(unique_values) == 2:\n",
    "                boundary_candidates.append((np.mean(unique_values), feature_index))\n",
    "            else:\n",
    "                for i in range(len(unique_values) - 1):\n",
    "                    boundary = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                    boundary_candidates.append((boundary, feature_index))\n",
    "        \n",
    "        if len(boundary_candidates) == 0:\n",
    "            return (None, None, False)\n",
    "            # raise Exception(\"No candidate boundaries found due to lack of variability in features\")\n",
    "        \n",
    "        for candidate in boundary_candidates:\n",
    "            boundaries.append((candidate[0], candidate[1], self.__gini_index_for_boundary(X_train, y_train, candidate[0], candidate[1])))\n",
    "            \n",
    "        boundaries.sort(key=self.__sorter)\n",
    "        \n",
    "        best_boundary = boundaries[0]\n",
    "        left_leaf = (len(np.unique(y_train[X_train[:, best_boundary[1]] <= best_boundary[0]])) == 1)\n",
    "        right_leaf = (len(np.unique(y_train[X_train[:, best_boundary[1]] > best_boundary[0]])) == 1)\n",
    "        is_leaf = left_leaf and right_leaf\n",
    "        \n",
    "        return (best_boundary[0], best_boundary[1], is_leaf)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def __gini_index_for_boundary(self, X_train, y_train, boundary, feature_num):\n",
    "        #classification assumed\n",
    "        \n",
    "        #calculate proportions of feature_num <= boundary for 1, proportion of feature_num > boundary for 2\n",
    "        # code pulled from https://stackoverflow.com/questions/67586928/how-can-you-find-what-percentage-of-a-numpy-array-has-some-value-x\n",
    "        # uniques, counts = np.unique(array, return_counts=True)\n",
    "        # percentages = dict(zip(uniques, counts * 100 / len(array)))\n",
    "    \n",
    "        # r1 is the subset of y masked by X[:,feature_num]<= or > the boundary, because we use it to find the proportions\n",
    "        R_1 = y_train[X_train[:,feature_num]<=boundary]\n",
    "        N_1 = R_1.shape[0]\n",
    "        proportions_1 = np.unique(R_1,return_counts=True)[1]/N_1\n",
    "        \n",
    "        R_2 = y_train[X_train[:,feature_num]>boundary]\n",
    "        N_2 = R_2.shape[0]\n",
    "        proportions_2 = np.unique(R_2,return_counts=True)[1]/N_2\n",
    "        \n",
    "        N = X_train.shape[0]\n",
    "        \n",
    "        #compute ginis\n",
    "        g_1 = 0\n",
    "        for prop in proportions_1:\n",
    "            g_1 = g_1 + prop*(1-prop)\n",
    "            \n",
    "        g_2 = 0\n",
    "        for prop in proportions_2:\n",
    "            g_2 = g_2 + prop*(1-prop)\n",
    "            \n",
    "        #return weighted average of ginis\n",
    "        return ((N_1/N)*(g_1))+((N_2/N)*(g_2))\n",
    "        \n",
    "    def __sorter(self, the_tuple):\n",
    "        return the_tuple[2]\n",
    "        \n",
    "    def predict(self, X_pred):\n",
    "        return np.array([self.__predict(x) for x in X_pred])\n",
    "    \n",
    "    def __predict(self, x_pred):\n",
    "        # For any vector of features given, transverses the tree in order to return the value\n",
    "        # x_pred is a vector\n",
    "        tolkein = self.root # could replace this with 'walker' or something else\n",
    "        while(type(tolkein)==type(self.root)):\n",
    "            if(x_pred[tolkein.var_num] <= tolkein.boundary):\n",
    "                tolkein = tolkein.left\n",
    "            else:\n",
    "                tolkein = tolkein.right\n",
    "        return tolkein\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdt = random_decision_tree()\n",
    "rdt.fit(X, y)\n",
    "predicted = rdt.predict(X)\n",
    "print(accuracy_score(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdt.root.var_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest = bagging_trees(random_decision_tree, 100)\n",
    "random_forest.fit(X, y)\n",
    "prediction = random_forest.predict(X)\n",
    "print(accuracy_score(y,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Plot the Decision Boundaries of the Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#87cefa', '#ff0000']\n",
    "cmap = ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=prediction, cmap=cmap, label='Predicted Labels')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('Predicted Labels')\n",
    "\n",
    "legend_labels = ['Not Purchased', 'Purchased']\n",
    "legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#87cefa', markersize=10),\n",
    "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#ff0000', markersize=10)]\n",
    "plt.legend(legend_markers, legend_labels, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 5),\n",
    "                     np.arange(y_min, y_max, 5))\n",
    "\n",
    "Z = rdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('Decision Boundary of Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and evaluate Decision Tree\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "dt_prediction = dt_classifier.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_prediction)\n",
    "print(f'Decision Tree Accuracy: {dt_accuracy}')\n",
    "\n",
    "# Train and evaluate custom Random Forest\n",
    "random_forest = bagging_trees(random_decision_tree, 100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "rf_prediction = random_forest.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_prediction)\n",
    "print(f'Custom Random Forest Accuracy: {rf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_memory = memory_profiler.memory_usage()[0]\n",
    "        result = func(*args, **kwargs)\n",
    "        end_memory = memory_profiler.memory_usage()[0]\n",
    "        memory_consumption = max(0, end_memory - start_memory)\n",
    "        return result, memory_consumption\n",
    "    return wrapper\n",
    "\n",
    "# Train and evaluate Decision Tree\n",
    "@measure_memory\n",
    "def evaluate_decision_tree():\n",
    "    start_time = time.time()\n",
    "    dt_classifier = DecisionTreeClassifier()\n",
    "    dt_classifier.fit(X_train, y_train)\n",
    "    dt_prediction = dt_classifier.predict(X_test)\n",
    "    dt_accuracy = accuracy_score(y_test, dt_prediction)\n",
    "    dt_runtime = time.time() - start_time\n",
    "    return dt_accuracy, dt_runtime\n",
    "\n",
    "dt_result, dt_memory_consumption = evaluate_decision_tree()\n",
    "print(f'Decision Tree Accuracy: {dt_result[0]}')\n",
    "print(f'Decision Tree Runtime: {dt_result[1]} seconds')\n",
    "print(f'Decision Tree Memory Consumption: {dt_memory_consumption} MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate custom Random Forest\n",
    "@measure_memory\n",
    "def evaluate_random_forest():\n",
    "    start_time = time.time()\n",
    "    random_forest = bagging_trees(random_decision_tree, 100)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    rf_prediction = random_forest.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_prediction)\n",
    "    rf_runtime = time.time() - start_time\n",
    "    return rf_accuracy, rf_runtime\n",
    "\n",
    "rf_result, rf_memory_consumption = evaluate_random_forest()\n",
    "print(f'Custom Random Forest Accuracy: {rf_result[0]}')\n",
    "print(f'Custom Random Forest Runtime: {rf_result[1]} seconds')\n",
    "print(f'Custom Random Forest Memory Consumption: {rf_memory_consumption} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_memory\n",
    "def evaluate_sklearn_random_forest():\n",
    "    start_time = time.time()\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    rf_prediction = rf_classifier.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_prediction)\n",
    "    rf_runtime = time.time() - start_time\n",
    "    return rf_accuracy, rf_runtime\n",
    "\n",
    "sklearn_rf_result, sklearn_rf_memory_consumption = evaluate_sklearn_random_forest()\n",
    "print(f'Scikit-learn Random Forest Accuracy: {sklearn_rf_result[0]}')\n",
    "print(f'Scikit-learn Random Forest Runtime: {sklearn_rf_result[1]} seconds')\n",
    "print(f'Scikit-learn Random Forest Memory Consumption: {sklearn_rf_memory_consumption} MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Decision Tree', 'Custom Random Forest', 'Scikit-learn Random Forest']\n",
    "memory_consumptions = [dt_memory_consumption, rf_memory_consumption, sklearn_rf_memory_consumption]\n",
    "\n",
    "plt.bar(labels, memory_consumptions, color=['red', 'blue', 'green'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Memory Consumption (MB)')\n",
    "plt.title('Memory Consumption Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = [dt_result[1], rf_result[1], sklearn_rf_result[1]]\n",
    "\n",
    "plt.bar(labels, runtimes, color=['red', 'blue', 'green'])\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Runtime (seconds)')\n",
    "plt.title('Runtime Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure CPU usage\n",
    "def measure_cpu():\n",
    "    return psutil.cpu_percent()\n",
    "\n",
    "@measure_memory\n",
    "# Function to measure CPU usage and memory usage during evaluation\n",
    "def evaluate_sklearn_random_forest(n_estimators):   \n",
    "    start_time = time.time()\n",
    "    rf_classifier = RandomForestClassifier(n_estimators)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    rf_prediction = rf_classifier.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_prediction)\n",
    "    rf_runtime = time.time() - start_time\n",
    "    cpu_usage = measure_cpu()\n",
    "    return rf_accuracy, rf_runtime, cpu_usage\n",
    "\n",
    "@measure_memory\n",
    "def evaluate_custom_random_forest(n_estimators):\n",
    "    start_time = time.time()\n",
    "    random_forest = bagging_trees(random_decision_tree, n_estimators)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    rf_prediction = random_forest.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_prediction)\n",
    "    rf_runtime = time.time() - start_time\n",
    "    cpu_usage = measure_cpu()\n",
    "    return rf_accuracy, rf_runtime, cpu_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    X = np.random.rand(size, 1)\n",
    "    y = np.random.randint(0, 2, size=size)  # Generating random labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = [10, 100, 1000, 10000, 100000]\n",
    "iterations = [10, 100, 1000, 10000]\n",
    "\n",
    "memory_usages = []\n",
    "runtimes = []\n",
    "cpu_usage = []\n",
    "accuracies = []\n",
    "\n",
    "memory_usages_custom = []\n",
    "runtimes_custom = []\n",
    "cpu_usage_custom = []\n",
    "accuracies_custom = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = len(iterations)\n",
    "progress_bar = tqdm(total=total_iterations)\n",
    "size = 1000\n",
    "for n_estimators in iterations:\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    results, memory = evaluate_sklearn_random_forest(n_estimators)\n",
    "    accuracy, runtime, cpu = results\n",
    "    memory_usages.append((size, n_estimators, memory))\n",
    "    runtimes.append((size, n_estimators, runtime))\n",
    "    cpu_usage.append((size, n_estimators, cpu))\n",
    "    accuracies.append((size, n_estimators, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = len(iterations)\n",
    "progress_bar = tqdm(total=total_iterations)\n",
    "size = 1000\n",
    "for n_estimators in iterations:\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    results, memory = evaluate_custom_random_forest(n_estimators)\n",
    "    accuracy, runtime, cpu = results\n",
    "    memory_usages_custom.append((size, n_estimators, memory))\n",
    "    runtimes_custom.append((size, n_estimators, runtime))\n",
    "    cpu_usage_custom.append((size, n_estimators, cpu))\n",
    "    accuracies_custom.append((size, n_estimators, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Custom RF accuracy comparison\n",
    "plt.plot([x[1] for x in accuracies_custom], [x[2] for x in accuracies_custom], marker='o', linestyle='-', color='blue', label='Custom RF')\n",
    "\n",
    "# Plot Scikit-learn RF accuracy comparison\n",
    "plt.plot([x[1] for x in accuracies], [x[2] for x in accuracies], marker='o', linestyle='-', color='red', label='Scikit-Learn RF')\n",
    "\n",
    "plt.title('Accuracy Comparison RF Implementations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Custom RF memory usage\n",
    "plt.plot([x[1] for x in memory_usages_custom], [x[2] for x in memory_usages_custom], marker='o', linestyle='-', color='blue', label='Custom RF')\n",
    "\n",
    "# Scikit-learn RF memory usage\n",
    "plt.plot([x[1] for x in memory_usages], [x[2] for x in memory_usages], marker='o', linestyle='-', color='red', label='Scikit-learn RF')\n",
    "\n",
    "plt.title('Memory Comparison RF Implementations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Scikit-learn RF runtime comparison\n",
    "plt.plot([x[1] for x in runtimes], [x[2] for x in runtimes], marker='o', linestyle='-', color='blue', label='Scikit-Learn RF')\n",
    "\n",
    "# Plot Custom RF runtime comparison\n",
    "plt.plot([x[1] for x in runtimes_custom], [x[2] for x in runtimes_custom], marker='o', linestyle='-', color='red', label='Custom RF')\n",
    "\n",
    "plt.title('Runtime Comparison RF Implementations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Runtime')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Scikit-learn RF CPU usage comparison\n",
    "plt.plot([x[1] for x in cpu_usage], [x[2] for x in cpu_usage], marker='o', linestyle='-', color='blue', label='Scikit-Learn RF')\n",
    "\n",
    "# Plot Custom RF CPU usage comparison\n",
    "plt.plot([x[1] for x in cpu_usage_custom], [x[2] for x in cpu_usage_custom], marker='o', linestyle='-', color='red', label='Custom RF')\n",
    "\n",
    "plt.title('CPU Usage Comparison RF Implementations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('CPU Usage (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usages = []\n",
    "runtimes = []\n",
    "cpu_usage = []\n",
    "accuracies = []\n",
    "\n",
    "memory_usages_custom = []\n",
    "runtimes_custom = []\n",
    "cpu_usage_custom = []\n",
    "accuracies_custom = []\n",
    "\n",
    "total_iterations = len(dataset_sizes)\n",
    "progress_bar = tqdm(total=total_iterations)\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    results, memory = evaluate_sklearn_random_forest(100)\n",
    "    accuracy, runtime, cpu = results\n",
    "    memory_usages.append((size, n_estimators, memory))\n",
    "    runtimes.append((size, n_estimators, runtime))\n",
    "    cpu_usage.append((size, n_estimators, cpu))\n",
    "    accuracies.append((size, n_estimators, accuracy))\n",
    "\n",
    "total_iterations = len(dataset_sizes)\n",
    "progress_bar = tqdm(total=total_iterations)\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    results, memory = evaluate_custom_random_forest(100)\n",
    "    accuracy, runtime, cpu = results\n",
    "    memory_usages_custom.append((size, n_estimators, memory))\n",
    "    runtimes_custom.append((size, n_estimators, runtime))\n",
    "    cpu_usage_custom.append((size, n_estimators, cpu))\n",
    "    accuracies_custom.append((size, n_estimators, accuracy))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in accuracies_custom], [x[2] for x in accuracies_custom], marker='o', linestyle='-', color='blue', label='Custom RF')\n",
    "plt.plot([x[0] for x in accuracies], [x[2] for x in accuracies], marker='o', linestyle='-', color='red', label='Scikit-Learn RF')\n",
    "plt.title('Accuracy Comparison RF Implementations')\n",
    "plt.xlabel('Dataset Sizes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Memory comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in memory_usages_custom], [x[2] for x in memory_usages_custom], marker='o', linestyle='-', color='blue', label='Custom RF')\n",
    "plt.plot([x[0] for x in memory_usages], [x[2] for x in memory_usages], marker='o', linestyle='-', color='red', label='Scikit-Learn RF')\n",
    "plt.title('Memory Comparison RF Implementations')\n",
    "plt.xlabel('Dataset Sizes')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Runtime comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in runtimes], [x[2] for x in runtimes], marker='o', linestyle='-', color='blue', label='Scikit-Learn RF')\n",
    "plt.plot([x[0] for x in runtimes_custom], [x[2] for x in runtimes_custom], marker='o', linestyle='-', color='red', label='Custom RF')\n",
    "plt.title('Runtime Comparison RF Implementations')\n",
    "plt.xlabel('Dataset Sizes')\n",
    "plt.ylabel('Runtime')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# CPU usage comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([x[0] for x in cpu_usage], [x[2] for x in cpu_usage], marker='o', linestyle='-', color='blue', label='Scikit-Learn RF')\n",
    "plt.plot([x[0] for x in cpu_usage_custom], [x[2] for x in cpu_usage_custom], marker='o', linestyle='-', color='red', label='Custom RF')\n",
    "plt.title('CPU Usage Comparison RF Implementations')\n",
    "plt.xlabel('Dataset Sizes')\n",
    "plt.ylabel('CPU Usage (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
